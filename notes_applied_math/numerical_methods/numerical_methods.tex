\documentclass[oneside,a4paper,11pt]{report}
\usepackage{fullpage}

\usepackage{../../info/packages}
\usepackage{../../info/nomenclature}

\usepackage{stmaryrd} % for \llbracket, \rrbracket

\title{Numerical Methods\thanks{Disclaimer: most of the work in this document is not original; instead, it consists of personal notes that folow from various books, such as Lomax, Pulliam, Zing; Larsson, Thomee; Hirsch; Leveque; Blazek; Laney; Moin.}}
\date{\today}
\author{Alejandro Campos}

\begin{document}
\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Introductory Concepts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
\item Analytical solution $u$ satisfies
\begin{equation}
D(u) = Au - f = 0\quad \text{in } \Omega,
\label{pde}
\end{equation}
where $A$ is the differential operator.

\item Discrete numerical solution $U_j$ satisfies 
\begin{equation}
D_h(U^n_j) = 0 \quad \text{for all } n, j ,
\end{equation}
where $D_h$ is the discrete operator for $D$.

\item Continuous numerical solution $v$ satisfies
\begin{equation}
D_hv = 0 \quad \text{in } \Omega,
\label{npde}
\end{equation}
Thus, $v(t_n, x_j) = U^n_j$ for all $n, j$.

\item Local truncation error 
\begin{equation}
l^{n+1}_j = u(t_{n+1}, x_j) - U^{n+1}_j,
\end{equation}
given that $U^n_j = u(t_n, x_j)$ for all $j$. 

\item Global truncation error
\begin{equation}
e^{n+1}_j = u(t_{n+1}, x_j) - U^{n+1}_j,
\end{equation}
given that $U^0_j = u(t_0, x_j)$ for all $j$.

\item Truncation error $\tau$: difference between the discrete and analytical equations when applied to a given continuous function $g$
\begin{equation}
\tau = D_hg - Dg.
\label{trunc}
\end{equation}

The truncation error allows us to compute global truncation errors. Also, consider the truncation error for the analytical solution, that is, $\tau = D_hu - Du$. Using equation (\ref{pde}), one obtains that $u$ is also the solution to $D_hu = \tau$. This means that the \textbf{analytical solution satisfies the numerical equation but with an additional source term equal to $\tau$}. Conversely, consider the truncation error for the function $v$, $\tau = D_hv - Dv$. Using equation (\ref{npde}), one obtains that $v$ is also a solution to $Dv = \tau$. This means that the \textbf{numerical solution satisfies the analytical equation but with an additional source term equal to $\tau$}.

\item Order of accuracy $r$ defined by $\tau^n = \mathcal{O}(\Delta t^r)$ as $\Delta t \to 0$ for ODE's and $\tau^n = \mathcal{O}(\Delta x^r)$ as $\Delta x \to 0$ for PDE's, where $\Delta t$ is related to $\Delta x$.

\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Numerical Solution of ODE's}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%########################################################################
\chapter{List of time integrators}
%########################################################################
Consider the IVP for $u=u(t)$
\begin{equation}
\label{eq:ode_generic}
\frac{du}{dt} = f(t, u) \quad \text{in } (0,\infty)
\end{equation}
with initial condition $u(0) = u^0$. We will discretize time into a set of finite values $t^n$, and express the numerical solution at a given time $t^n$ as $U^n$. 

%----------------------------------------------------------------------------------------------------------------------------
\section{Explicit}
%----------------------------------------------------------------------------------------------------------------------------
\begin{itemize}

\item Explicit Euler method (Forward Euler method)
\begin{equation}
u^{n+1} = u^n + \Delta t f(t_n, u^n) \qquad \text{1\textsuperscript{st} O.A.}
\end{equation}

\item Explicit Midpoint (Modified Euler, 2\textsuperscript{nd} order RK)
\begin{align}
    u^{n+1/2} &= u^n + \frac{\Delta t}{2} f(t_n, u^n) \nonumber \\
    u^{n+1}   &= u^n + \Delta t f(t_{n+1/2}, u^{n+1/2})
\end{align}

\end{itemize}

%----------------------------------------------------------------------------------------------------------------------------
\section{Implicit}
%----------------------------------------------------------------------------------------------------------------------------
\begin{itemize}

\item Implicit Euler (Backward Euler)
\begin{equation}
u^{n+1} = u^n + \Delta t f(t_{n+1}, u^{n+1}) \qquad \text{1\textsuperscript{st} O.A.}
\end{equation}

\item Implicit Trapezoidal (Crank-Nicholson)
\begin{equation}
u^{n+1} = u^n + \frac{\Delta t}{2} [ f(t_n, u^n) + f(t_{n+1}, u^{n+1}) ] \qquad \text{2\textsuperscript{nd} O.A.}
\end{equation}

\item Implicit Midpoint
\begin{equation}
    u^{n+1} = u^n + \Delta t f\left (t_{n+1/2}, \frac{1}{2} \left (u^n + u^{n+1} \right ) \right )
\end{equation}

\end{itemize}

%----------------------------------------------------------------------------------------------------------------------------
\section{Predictor-Corrector}
%----------------------------------------------------------------------------------------------------------------------------
\begin{itemize}

\item Heun's (Explicit Trapezoidal, Improved Euler)
\begin{align}
\tilde{u}^{n+1} &= u^n + \Delta t f(t_n, u^n) \nonumber \\
u^{n+1}   &= u^n + \frac{\Delta t}{2} [ f(t_n, u^n) + f(t_{n+1}, \tilde{u}^{n+1}) ] \quad \text{2 \textsuperscript{nd} O.A.}
\end{align}

\end{itemize}

%----------------------------------------------------------------------------------------------------------------------------
\section{Runge-Kutta}
%----------------------------------------------------------------------------------------------------------------------------
\begin{itemize}
\item 4\textsuperscript{th} order Runge-Kutta
\begin{align}
k_1 &= f(t_n, u^n) \nonumber \\
k_2 & = f(t_{n+1/2}, u^n + \frac{\Delta t}{2} k_1) \nonumber \\
k_3 & = f(t_{n+1/2}, u^n + \frac{\Delta t}{2} k_2) \nonumber \\
k_4 & = f(t_n, u^n + \Delta t k_3) \nonumber \\
u^{n+1} &= u^n + \frac{\Delta t}{6} (k_1 + 2k_2 + 2k_3 + k_4)
\end{align}
\end{itemize}

%----------------------------------------------------------------------------------------------------------------------------
\section{Multi-Step}
%----------------------------------------------------------------------------------------------------------------------------
\begin{itemize}
\item Leapfrog: consider the system of equations for $x=x(t)$ and $v=v(t)$
\begin{equation}
    \frac{dx}{dt} = v \qquad \frac{dv}{dt} = f(x).
\end{equation}
The Leapfrog method is a staggered scheme defined as follows
\begin{align}
    v^{n+1/2} &= v^{n-1/2} + f(x^n) \Delta t \\
    x^{n+1} &= x^{n} + v^{n+1/2} \Delta t
\end{align}

\item Adams-Bashforth
\end{itemize}

%----------------------------------------------------------------------------------------------------------------------------
\section{Multi-Rate}
%----------------------------------------------------------------------------------------------------------------------------
Split the right-hand side of \cref{eq:ode_generic} as follows
\begin{equation}
    \label{eq:ode_generic_split}
    \frac{du}{dt} = f_f(t,u) + f_s(t,u) \qquad \text{in } (0,\infty),
\end{equation}
where $f_f(t,u)$ is the fast source and $f_s(t,u)$ the slow source. Also introduce fast and slow functions $u_f=u_f(t)$ and $u_s=u_s(t)$ that satisfy 
\begin{align}
    \frac{du_f}{dt} = f_f(t, u), \nonumber \\
    \frac{du_s}{dt}= f_s(t, u).
\end{align}
Plugging the above two in \cref{eq:ode_generic_split} leads to $u = u_f + u_s$. For the sake of clarity, we'll explicitly denote the fast and slow functional arguments in the sources. Thus, the equations are written as
\begin{align}
    \frac{du_f}{dt} = f_f(t,u_f, u_s), \nonumber \\
    \frac{du_s}{dt}= f_s(t,u_f,u_s).
\end{align}
Below, we describe how to solve this system of equations using an implicit-trapezoidal multi-rate Runge-Kutta scheme.

Introduce the auxiliary variable $v=v(\theta)$ which is also split into fast and slow parts, that is $v=v_f+v_s$, where $v_f=v_f(\theta)$ and $v_s=v_s(\theta)$. The algorithm is then defined as follows
\begin{itemize}
    \item Set $v_f(0) = u^n_f$, $v_s(0) = u^n_s$.
    \item Iterate $v_f$ and $v_s$ forward in time from $\theta=0$ up to $\theta=\Delta t$ by solving the following equations
    \begin{align}
        \frac{dv_f}{d\theta} &= f_f(t_n + \theta, v_f, v_s), \label{eq:ode_mr_v_fast} \\
        \frac{dv_s}{d\theta} &= f_s(t_n, v_f(0), v_s(0)) \label{eq:ode_mr_v_slow}.
    \end{align}
    Note that the right-hand side of \cref{eq:ode_mr_v_slow} is constant so integrating analytically we get
    \begin{equation}
        \label{eq:ode_mr_v_slow_sol}
        v_s = v_s(0) + \theta f_s(t_n, v_f(0), v_s(0)) .
    \end{equation}
    \Cref{eq:ode_mr_v_fast} is solved using an explicit Runge-Kutta scheme.
    \item Set $u^{n+1}_f = v_f(\Delta t)$.
    \item Obtain $u^{n+1}_s$ by solving the implicit trapezoidal equation below
    \begin{equation}
        u^{n+1}_s = u^n_s + \frac{\Delta t}{2} \left[ f_s(t_n, u^n_f, u^n_s) + f_s(t_{n+1}, u^{n+1}_f, u^{n+1}_s) \right].
    \end{equation}
    Note that we can re-write \cref{eq:ode_mr_v_slow_sol} as $v_s = u^n_s + \theta f_s(t_n, u^n_f, u^n_s)$. Using this to replace the first term in the right-hand side above we get
    \begin{equation}
        u^{n+1}_s = v_s - \theta f_s(t_n, u^n_f, u^n_s) + \frac{\Delta t}{2} \left[ f_s(t_n, u^n_f, u^n_s) + f_s(t_{n+1}, u^{n+1}_f, u^{n+1}_s) \right].
    \end{equation}
    Evaluating this at $\theta=\Delta t$ we get
    \begin{equation}
        u^{n+1}_s = v_s(\Delta t) - \frac{\Delta t}{2} f_s(t_n, u^n_f, u^n_s) + \frac{\Delta t}{2} f_s(t_{n+1}, u^{n+1}_f, u^{n+1}_s).
    \end{equation}
    Adding $u^{n+1}_f$ to both sides we get
    \begin{equation}
        u^{n+1} = v(\Delta t) - \frac{\Delta t}{2} f_s(t_n, u^n_f, u^n_s) + \frac{\Delta t}{2} f_s(t_{n+1}, u^{n+1}_f, u^{n+1}_s).
    \end{equation}

    
\end{itemize} 


%########################################################################
\chapter{Solving non-linear equations}
%########################################################################
The implicit schemes result in non-linear equations that require some sort of non-linear solver. For this chapter, we'll assume the governing ODE is
\begin{equation}
\label{eq:time_stepping_ode}
    \frac{du}{dt} = f(u) \qquad \text{in }(0,\infty),
\end{equation}
or, for a system of equations,
\begin{equation}
\label{eq:time_stepping_ode_system}
    \frac{d\uvec}{dt} = \fvec(\uvec) \qquad \text{in }(0,\infty).
\end{equation}

%----------------------------------------------------------------------------------------------------------------------------
\section{Newton's method}
%----------------------------------------------------------------------------------------------------------------------------
The Euler scheme requires finding $u^{n+1}$ so that the following is satisfied 
\begin{equation}
    u^{n+1} = u^n + \Delta t f(u^{n+1}).
\end{equation}
That is, we need to find the root of the nonlinear equation 
\begin{equation}
\label{eq:backward_euler_root}
    g(x) = x - u^n - \Delta t f(x).
\end{equation}
Newton's method finds the root of a non-linear equation $g(x)$ by iterating through $m$ as follows
\begin{equation}
    x^{m+1}=x^m - \frac{g(x^m)}{g'(x^m)}.
\end{equation}
Applying Newton's method to \cref{eq:backward_euler_root}, we have
\begin{equation}
\label{eq:backward_euler_newtons_method_scalar}
    x^{m+1} = x^m - \frac{x^m - u^n - \Delta t f(x^m)}{1 - \Delta t f'(x^m)},
\end{equation}
The initial value for the non-linear solver is $x^0 = u^n$. Thus, using the above, $u^{n+1}$ is approximated by $x^m$ as $m\to\infty$. Note that this is only one example of what is referred to as a fixed-point iteration. There are other fixed-point-iteration methods that can be used to solve \cref{eq:backward_euler_root} that are not Newton's method.

As a side note, let's assume $u^{n+1}$ is approximated sufficiently well by $x^1$, that is, only one Newton iteration is needed. Then we have
\begin{equation}
    u^{n+1} = u^n + \frac{\Delta t f(u^n)}{1 - \Delta t f'(u^n)}, 
\end{equation}
which we re-write as
\begin{equation}
\label{eq:backward_euler_pseudotime_stepping_scalar}
    \left ( \frac{1}{\Delta t} - f'(u^n) \right ) \Delta u = f(u^n),
\end{equation}
where $\Delta u = u^{n+1} - u^n$. The above is the approximation that gets used for pseudo-time stepping, so that the Backward Euler scheme can converge to the steady state solution as one iterates through $n$.

To solve for the root of a system of non-linear equations $\gvec(\xvec)$, Newton's method is as follows
\begin{equation}
    \xvec^{m+1} = \xvec^m - \Jvec^{-1}(\xvec^m) \gvec(\xvec^m). 
\end{equation}
In the above, $\Jvec^{-1}(\xvec)$ is the inverse of the Jacobian matrix $\Jvec(\xvec)$, which is given by
\begin{equation}
    \Jvec(\xvec) = \frac{\partial \gvec(\xvec)}{\partial \xvec} = \begin{pmatrix} 
    \frac{\partial g_1(\xvec)}{\partial x_1} & \frac{\partial g_1(\xvec)}{\partial x_2} & \dots \\
    \frac{\partial g_2(\xvec)}{\partial x_1} & \frac{\partial g_2(\xvec)}{\partial x_2} & \\
    \vdots &        & \ddots
    \end{pmatrix}.
\end{equation}
For the backward Euler scheme, the equivalent of \cref{eq:backward_euler_newtons_method_scalar} would be
\begin{equation}
\label{eq:backward_euler_newtons_method_system}
    \xvec^{m+1} = \xvec^m - \left ( \Ivec - \Delta t \left . \frac{\partial \fvec(\xvec)}{\partial \xvec} \right |_{\xvec = \xvec^m} \right )^{-1} \left ( \xvec^m - \uvec^n - \Delta t \fvec(\xvec^m) \right ),
\end{equation}
and the equivalent of \cref{eq:backward_euler_pseudotime_stepping_scalar} would be
\begin{equation}
\label{eq:backward_euler_pseudotime_stepping_system}
    \left ( \frac{\Ivec}{\Delta t} - \left . \frac{\partial \fvec(\xvec)}{\partial \xvec} \right |_{\xvec = \uvec^n} \right ) \Delta \uvec = \fvec(\uvec^n),
\end{equation}
where $\Delta \uvec = \uvec^{n+1} - \uvec^n$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Finite Difference for PDE's}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%########################################################################
\chapter{Introduction}
%########################################################################
%---------------------------------------------------------------
\section{Finite difference formulas}
%---------------------------------------------------------------
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Forward & Backward & Central & Central (2nd order)\\
\hline
\parbox{3cm}{\[\partial U_j = \frac{U_{j+1} - U_{j}}{h}\]} &
\parbox{3cm}{\[\bar{\partial} U_j = \frac{U_{j} - U_{j-1}}{h}\]} & 
\parbox{4cm}{\[\hat{\partial} U_j = \frac{U_{j+1} - U_{j-1}}{2h}\]} &
\parbox{5cm}{\[\partial \bar{\partial} U_j = \frac{U_{j+1} - 2U_j + U_{j-1}}{h^2}\]}\\
\hline
\end{tabular}
\end{center}

%---------------------------------------------------------------
\section{Fourier analysis}
%---------------------------------------------------------------
This analysis is based on a test function that is periodic and thus has the following form
\begin{equation}
    u(x) = \sum_{n = -\infty}^{n=\infty} \hat{u}_n e^{i \kappa_n x}.
\end{equation}
We are interested in using the different numerical methods to compute the resulting approximate derivatives of the generic mode $e^{i \kappa x}$. We note that one can then combine the approximate derivatives of all the modes to obtain the approximate derivative of our test function u.

The analytically derivative of our generic mode $e^{i\kappa x}$ is $i\kappa e^{i\kappa x}$. We now assume that the approximate derivatives at a point $x_j$ obtained using numerical schemes will be of the form $i\kappa^* e^{i\kappa x_j}$, where $\kappa^*$ is a modified wave number. The closer $\kappa^*$ is to $\kappa$ the higher the accuracy of the numerical scheme. For example, for the central first order scheme 
\begin{equation}
\delta U_j = \frac{ U_{j+1} - U_{j-1} }{ 2h }
\end{equation}
we have
\begin{equation}
    i\kappa^*e^{i\kappa x_j} = \frac{ e^{i\kappa (x_j+h) } - e^{ i \kappa (x_j-h)  } }{2h}
\end{equation}
which leads to
\begin{equation}
    \kappa^* = \frac{ \sin \kappa h }{h}.
\end{equation}

For the sixth-order compact scheme
\begin{equation}
    \alpha \delta U_{j-1} + \delta U_j + \alpha \delta U_{j+1} = \frac{a}{2h} \left( U_{j+1} - U_{j-1} \right) + \frac{b}{4h} \left ( U_{j+2} - U_{j-2} \right)
\end{equation}
we have
\begin{equation}
    \alpha i \kappa^* e^{i \kappa (x_j -h) } + i\kappa^* e^{i \kappa x_j} + \alpha i \kappa^* e^{i \kappa (x_j + h)} = \frac{a}{2h} \left [ e^{i \kappa (x_j+h) } - e^{ i \kappa (x_j-h) } \right ] + \frac{b}{2h} \left [ e^{i \kappa (x_j+2h) } - e^{ i \kappa (x_j-2h) } \right ]
\end{equation}
which leads to
\begin{equation}
    \kappa^* = \frac{ \frac{a}{h} \sin \kappa h + \frac{b}{2h} \sin 2\kappa h}{1 + 2\alpha \cos \kappa h}.
\end{equation}

A spectral method, on the other hand, will explicitly express the derivative of a mode as $i \kappa e^{i \kappa x_j}$, up to the last mode $\kappa = \frac{2 \pi}{L} \frac{N}{2}$, and will not be able to capture higher modes. Thus,
\begin{equation}
    \kappa^* = \begin{cases} \kappa \quad \text{for } \frac{2 \pi}{L} \left (-\frac{N}{2} +1 \right ) \le \kappa \le \frac{2 \pi}{L} \frac{N}{2} \\ 0 \quad \text{o.w.} \end{cases}
\end{equation}

%########################################################################
\chapter{Elliptic}
%########################################################################
Define the following norm: $|U|_S = \max_{x_j \in S}|U_j|$.\\
For $Au = -au''+cu=f$ in $\Omega=(0,1)$, where $a(x)>0$ and $c(x)\ge0$, with boundary conditions $u(0) = U_0$ and $u(1) = U_m$, and $A_h = -a_j\partial\bar{\partial}U_j + c_jU_j$:
\begin{itemize}
\item \textbf{Lemma 4.2} $$|U|_{\bar{\Omega}} \le \max\{|U_0|,|U_M|\} + C|A_hU|_{\Omega}$$

\item \textbf{Theorem 4.1} The error bound follows,
$$|U-u|_\Omega \le Ch^2||u||_C^4$$
\end{itemize}
For $Au = -\Delta u=f$ in $\Omega= (0,1)\times(0,1)$, with boundary doncitions $u=0$ in $\Gamma$, and $A_h = -\Delta_h = -\partial_1\bar{\partial_1}U_j - \partial_2\bar{\partial_2}U_j$:
\begin{itemize}
\item \textbf{Lemma 4.4} $$|U|_{\bar{\Omega}} \le |U|_\Gamma + C|\Delta_hU|_\Omega$$

\item \textbf{Theorem 4.2} The error bound follows,
$$|U-u|_\Omega \le Ch^2||u||_C^4$$
\end{itemize}

%########################################################################
\chapter{Parabolic}
%########################################################################
For $u_t = u_{xx}$ in $\mathbf{R}\times\mathbf{R_+}$, with initial condition $u(\cdot,0) = v$ in $\mathbf{R}$.
\begin{itemize}
\item Each scheme can be associated with its discrete solution operator $E_k$, defined in either of the following two ways, where $U^n_{j}$ is defined only at mesh points, and $u^n(x)$ is the corresponding numerical solution defined over all space,
$$U_j^{n+1} = (E_kU^n)_j = \sum_p a_pU_{j-p}^n$$
$$u^{n+1}(x) = (E_ku^n)(x) = \sum_p a_pu^n(x-x_p)$$

\item Repeated application yields $U^n_j = (E_k^nV)_j$ or $u^n(x) = (E_k^nv)(x)$, where $V_i$ and $v(x)$ are the initial conditions, specified at mesh points and over all space, respectively.

\item The \textbf{stability} of $E_k$ (and hence the associated scheme) is defined by $||U^n||_{l_p} = ||E_k^nV||_{l_p} \le ||V||_{l_p}$ and $||u^n||_{L_p} = ||E_k^nv||_{L_p} \le ||v||_{L_p}$. 

\item The \textbf{symbol} or characteristic polynomial of $E_k$ is defined as follows,
$$\tilde{E}(\xi) = \sum_p a_pe^{-i p \xi}.$$
If we make use of the following Fourier series and Fourier transform,
$$\hat{V}(\xi) = h \sum_{j=-\infty}^{\infty}V_j e^{-ij\xi}$$
$$\hat{v}(\xi) = \int_{-\infty}^{\infty}v(x)e^{-ix\xi}dx$$
then we obtain $(E_kV)\hat{}(\xi) = \tilde{E}(\xi)\hat{V}(\xi)$ and $(E_kv)\hat{}(\xi) = \tilde{E}(h\xi)\hat{v}(\xi)$, which for repeated application leads to,
$$(E^n_kV)\hat{}(\xi) = \tilde{E}^n(\xi)\hat{V}(\xi)$$
$$(E^n_kv)\hat{}(\xi) = \tilde{E}^n(h\xi)\hat{v}(\xi).$$

\item Using Parseval's theorem one arrives at the \textbf{von Neumann's stability condition}, namely, $|\tilde{E}(\xi)| \le 1|$ for all $\xi$ is sufficient for stability in $l_2$ and $L_2$, and necessary for stability in $l_\infty$.
 
\item The \textbf{order of accuracy} $r$ is defined by $\tau^n = \mathcal{O}(h^r)$ as $h \to 0$. Since $u^{n+1}(x) = E_ku^n(x) + k\tau^n(x)$, where $u^n(x)$ is now the analytical solution and $k$ is the time step, then the order of accuracy is also obtained from $u^{n+1}(x) - E_ku^n(x) = k\mathcal{O}(h^r)$. This expression for order of accuracy is equivalent to $\tilde{E}(\xi) = e^{-\lambda \xi^2} + \mathcal{O}(|\xi|^{r+2})$, as $\xi \to 0$.
\end{itemize}

%########################################################################
\chapter{Hyperbolic}
%########################################################################
For $u_t = au_x$ in $\mathbf{R}\times\mathbf{R_+}$, with initial condition $u(\cdot,0) = v$ in $\mathbf{R}$:
\begin{itemize}
\item As for parabolic equations, von Neumann's condition $|\tilde{E}(\xi)| \le 1$ for all $\xi$ is a necessary and sufficient condition for stability in the $L_2$ norm. 

\item Order of accuracy is defined in the same manner as for the parabolic case, except that now the symbol of $E_k$ is given by $\tilde{E}(\xi) = e^{ia\lambda\xi} + \mathcal{O}(|\xi|^{r+1})$ as $\xi \to 0$.

\item \textbf{CFL condition} for stability: a necessary condition for stability is that the domain of dependence of the finite difference scheme at $(x,t)$ contains the domain of dependence of the continuous problem.

\item The Friedrichs scheme (first order accurate) follows,
$$(E_kU^n)(x) = 1/2(1+a\lambda)U^n(x+h) + 1/2(1-a\lambda)U^n(x-h)$$

\item The Lax-Wendroff scheme (second order accurate) follows,
$$(E_kU^n)(x) = 1/2(a^2\lambda^2+a\lambda)U^n(x+h) + (1-a^2\lambda^2)U^n(x) + 1/2(a^2\lambda^2-a\lambda)U^n(x-h)$$
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Finite Volume for PDE's}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%########################################################################
\chapter{Introduction}
%########################################################################

%---------------------------------------------------------------
\section{Some schemes for high-speed flows}
%---------------------------------------------------------------
\begin{itemize}
    \item Central
    \begin{itemize}
        \item Jameson
    \end{itemize}
    \item Upwinding
    \begin{itemize}
        \item Flux Vector Splitting
        \begin{itemize}
             \item Steger-Warming
             \item AUSM
        \end{itemize}
        \item Flux Difference Splitting
        \begin{itemize}
            \item Godunov
            \item Roe
       \end{itemize}
        \item Total Variation Diminishing
    \end{itemize}
\end{itemize}

%########################################################################
\chapter{Elliptic}
%########################################################################

%########################################################################
\chapter{Parabolic}
%########################################################################

%########################################################################
\chapter{Hyperbolic}
%########################################################################

%---------------------------------------------------------------
\section{One-dimensional case}
%---------------------------------------------------------------
Consider the hyperbolic equation
\begin{equation}
\frac{\partial u}{\partial t} + \frac{\partial f}{\partial x} = 0,
\end{equation}
where $f = f(u)$ is the flux of $u$. The finite volume method consists on discretizing the spatial domain into volumes, which we will denote by the index $i$, and which are defined by $x \in [x_{i-1/2}, x_{i+1/2}]$, where $x_{i-1/2}$ and $x_{i+1/2}$ represent the boundaries of the finite volume $i$.

We now proceed by averaging the equation over each control volume, that is
\begin{equation}
\frac{1}{\Delta x_i} \int_{x_{i-1/2}}^{x_{1+1/2}} \frac{\partial u}{\partial t} dx + \frac{1}{\Delta x_i} \int_{x_{i-1/2}}^{x_{1+1/2}} \frac{\partial f}{\partial x} dx = 0,
\end{equation}
where $\Delta x_i = x_{i+1/2} - x_{i-1/2}$. Moving the time derivative and using the Divergence theorem, the above becomes
\begin{equation}
\frac{d}{d t} \left( \frac{1}{\Delta x_i} \int_{x_{i-1/2}}^{x_{i+1/2}} u \,dx \right) + \frac{1}{\Delta x_i} ( f_{i+1/2} - f_{i-1/2} ) = 0,
\end{equation}
where $f_{i \pm 1/2}$ is the flux evaluated at $x_{i \pm 1/2}$. The spatially-discrete numerical solution then satisfies 
\begin{equation}
\frac{d U_i}{dt} + \frac{1}{\Delta x_i} ( F_{i+1/2} - F_{i-1/2} ) = 0,
\end{equation}
where $U_i = U_i(t)$ is the discrete solution at some specific point within the finite volume, and is used to approximate the average of $u$. $F_{i \pm 1/2}$ are the so-called numerical fluxes.

Godunov Scheme:
$$ F_{j+1/2}^n = \left\{ \begin{array}{lll}  \min_{U_j \le u \le U_{j+1}} f(u) & \text{for} & U_j < U_{j+1}\\
                                                                               \min_{U_{j+1} \le u \le U_j} f(u) & \text{for} & U_j > U_{j+1} \end{array}\right. $$
Lax-Friedrichs:
$$F_{j+1/2}^n = \frac{h}{2k}(U_j-U_{j+1}+\frac{1}{2}(f(U_j)+f(U_{j+1}))$$                                                                               

%---------------------------------------------------------------
\section{Multi-dimensional case}
%---------------------------------------------------------------
We follow a similar approach for the multidimensional case. Consider a generic conservation equation
\begin{align}
\frac{\partial w_i}{\partial t} + \frac{\partial f^{(c)}_{ij}}{\partial x_j} = \frac{\partial f^{(v)}_{ij}}{\partial x_j} + q_i
\end{align}
where $w_i$ is the vector of conservative variables, $f^{(c)}_{ij}$ the convective flux tensor, $f^{(v)}_{ij}$ the viscous flux tensor, and $q_i$ some heat source. 

Averaging the equation over a generic finite volume denoted by the index $I$, one obtains
\begin{equation}
\frac{d}{d t} \left ( \frac{1}{\Omega_I} \int_{\Omega_I} w_i \, dV \right)+  \frac{1}{\Omega_I} \int_{\delta \Omega_I} f^{(c)}_i \,dS = \frac{1}{\Omega_I} \int_{\delta \Omega_I} f^{(v)}_i \, dS + \frac{1}{\Omega_I} \int_{\Omega_I} q_i \,dV,
\end{equation}
where $f^{(c)}_i = f^{(c)}_{ij} n_j$ and $f^{(v)}_i = f^{(v)}_{ij} n_j$ are the vectors of convective and viscous fluxes, respectively. In vector notation, this is written as
\begin{equation}
\frac{d }{d t} \left ( \frac{1}{\Omega_I} \int_{\Omega_I} \wvec \,dV \right) +  \frac{1}{\Omega_I} \int_{\delta \Omega_I} \fvec^{(c)} \,dS = \frac{1}{\Omega_I} \int_{\delta \Omega_I} \fvec^{(v)} \, dS + \frac{1}{\Omega_I} \int_{\Omega_I} \qvec \,dV.
\end{equation}
A specific example is the Navier-Stokes equations, for which we have
\begin{equation}
\wvec = \begin{bmatrix} \rho \\ \rho u \\ \rho v \\ \rho w \\ \rho E \end{bmatrix}  \qquad
\fvec^{(c)} = \begin{bmatrix} \rho (u_j n_j) \\ \rho u (u_j n_j) + p n_1 \\ \rho v (u_j n_j) + p n_2 \\ \rho w (u_j n_j) + p n_3 \\ \rho \left (E + \frac{p}{\rho} \right) (u_j n_j) \end{bmatrix} \qquad
\fvec^{(v)} = \begin{bmatrix} 0 \\ \tau_{1j}n_j \\ \tau_{2j}n_j \\ \tau_{3j}n_j \\ u_i \tau_{ij}n_j +  \kappa \frac{\partial T}{\partial x_j} n_j \end{bmatrix} \qquad
\qvec = \begin{bmatrix} 0 \\ \rho g_1 \\ \rho g_2 \\ \rho g_3 \\  \rho u_i g_i \end{bmatrix}.
\end{equation} 

The spatially-discrete numerical solution then satisfies
\begin{equation}
\frac{d \Wvec_I}{dt} +  \frac{1}{\Omega_I} \sum_{K \in N(I)} \Fvec^{(c)}_K \,\Delta S_K = \frac{1}{\Omega_I} \sum_{K \in N(I)} \Fvec^{(v)}_K \, \Delta S_K + \Qvec_I
\end{equation}
where $\Wvec_I = \Wvec_I(t)$ is the numerical solution at some specific point within the finite volume, and is used to approximate the average of the vector $\wvec$. The faces of the finite volumes are indexed, and the set $N(I)$ consists of the indices of the faces of the finite volume $I$. The variables $\Fvec^{(c)}_K = \Fvec^{(c)}_K(\Wvec_1, \Wvec_2, \dots)$ and $\Fvec^{(v)}_K = \Fvec^{(v)}_K(\Wvec_1, \Wvec_2,\dots)$ are the numerical fluxes, and $\Delta S_K$ the surface area, for face $K$. The averaged source term $\Qvec_I = \Qvec_I(\Wvec_I)$ is approximated as the source vector $\qvec$ evaluated using $\Wvec_I$. The above is typically rewritten as
\begin{equation}
\label{eq:finite_vol_ode}
\frac{d \Wvec_I}{dt} = -\frac{1}{\Omega_I} \Rvec_I,
\end{equation}
where the residual $\Rvec_I = \Rvec_I(\Wvec_1, \Wvec_2, \dots)$ is given by
\begin{equation}
\Rvec_I = \sum_{K \in N(I)} \Fvec^{(c)}_K \,\Delta S_K - \sum_{K \in N(I)} \Fvec^{(v)}_K \, \Delta S_K - \Qvec_I\, \Omega_I
\end{equation}
The Jacobian is then
\begin{equation}
    \frac{\partial \Rvec_I}{\partial \Wvec_J} = \sum_{K \in N(I)} \frac{\partial \Fvec^{(c)}_K}{\partial \Wvec_J} \,\Delta S_K - \sum_{K \in N(I)} \frac{\partial \Fvec^{(v)}_K}{\partial \Wvec_J} \, \Delta S_K - \frac{\partial \Qvec_I}{\partial \Wvec_J}\, \Omega_I
\end{equation}
Note the the Jacobian is only non zero when $J \in M(I)$, where $M(I)$ represents the set of all finite volumes that the numerical fluxes of finite volume $I$ depend on. 

\subsection{An example from turbulence modeling}
%---------------------------------------------------------------
Consider the SST turbulence model. The two transport equations solved by the model are
\begin{equation}
    \frac{\partial \rho k}{\partial t} + \frac{\partial \rho k u_j}{\partial x_j} = P - \beta^* \rho w k + \frac{\partial}{\partial x_j} \left [ \left ( \mu + \sigma_k \mu_t \right ) \frac{\partial k}{\partial x_j} \right]
\end{equation}
\begin{equation}
    \frac{\partial \rho w}{\partial t} + \frac{\partial \rho w u_j}{\partial x_j} = \frac{\gamma}{\nu_t} P - \beta \rho w^2 + \frac{\partial}{\partial x_j} \left [ \left ( \mu + \sigma_w \mu_t \right) \frac{\partial w}{\partial x_j} \right] + 2 (1-F_1) \frac{\rho \sigma_{w2}}{w} \frac{\partial k}{\partial x_j}\frac{\partial w}{\partial x_j}
\end{equation}

The vector of conservative variables is $\wvec = [\rho k, \rho w]^T$. The convective and viscous flux vectors are 
\begin{equation}
    \fvec^{(c)} = \begin{bmatrix} \rho k (u_j n_j) \\ \rho w (u_j n_j) \end{bmatrix}
    \qquad
    \fvec^{(v)} = \begin{bmatrix} (\mu + \sigma_k \mu_t) \frac{\partial k}{\partial x_j} n_j \\ (\mu + \sigma_w \mu_t) \frac{\partial w}{\partial x_j} n_j \end{bmatrix}.
\end{equation}
The source $\qvec$ is
\begin{equation}
\label{eq:sst_source}
    \qvec = \begin{bmatrix} P - \beta^* \rho w k \\ \frac{\gamma}{\nu_t} P - \beta \rho w^2 + 2 (1-F_1) \frac{\rho \sigma_{w2}}{w} \frac{ \partial k}{\partial x_j} \frac{\partial w}{\partial x_j} \end{bmatrix}
\end{equation}

Convective numerical fluxes:
A simple upwinding scheme is as follows
\begin{equation}
    \Fvec^{(c)}_K  = \begin{bmatrix} (\rho k)_l a_0 + (\rho k)_r a_1 \\
                                     (\rho w)_l a_0 + (\rho w)_r a_1
                     \end{bmatrix}
\end{equation}
where subscripts $l$ and $r$ denote values at the center nodes of the control volumes to the left and right of the $K$\textsuperscript{th} control surface, respectively. The variables $a_0$ and $a_1$ are given by
\begin{equation}
    a_0 = \begin{cases} q_{lr} & \text{if }q_{lr} > 0 \\
                        0 & \text{o.w.}
    \end{cases}
\qquad
    a_1 = \begin{cases} 0 & \text{if }q_{lr} > 0 \\
                        q_{lr} & \text{o.w.}
    \end{cases}
\end{equation}
where
\begin{equation}
    q_{lr} = \frac{ (\uvec_l \cdot \nvec) + (\uvec_r \cdot \nvec) }{2}.
\end{equation}
The corresponding Jacobian is
\begin{equation}
    \frac{\partial \Fvec^{(c)}_K}{\partial \Wvec_J} = \begin{bmatrix}
    \dfrac{\partial \Fvec^{(c)}_K(1)}{\partial \Wvec_J(1)} & \dfrac{\partial \Fvec^{(c)}_K(1)}{\partial \Wvec_J(2)} \\
    \dfrac{\partial \Fvec^{(c)}_K(2)}{\partial \Wvec_J(1)} & \dfrac{\partial \Fvec^{(c)}_K(2)}{\partial \Wvec_J(2)} \end{bmatrix} = \begin{bmatrix}
    a_0 \delta_{lJ} + a_1 \delta_{rJ} & 0 \\
    0 & a_0 \delta_{lJ} + a_1 \delta_{rJ} \end{bmatrix}.
\end{equation}

Viscous numerical fluxes:
the viscous fluxes can be discretized as follows
\begin{equation}
    \Fvec^{(v)}_K = \begin{bmatrix}
    \left ( \mu + \sigma_k \mu_t \right)_{avg} \left [ \frac{k_r - k_l}{l_{lr}} \tvec_{lr} + \nabla k_{avg} - \left ( \nabla k_{avg} \cdot \tvec_{lr} \right) \tvec_{lr} \right ] \cdot \nvec \\ 
    \left ( \mu + \sigma_\omega \mu_t \right)_{avg} \left [ \frac{\omega_r - \omega_l}{l_{lr}} \tvec_{lr} + \nabla \omega_{avg} - \left ( \nabla \omega_{avg} \cdot \tvec_{lr} \right) \tvec_{lr} \right ] \cdot \nvec
    \end{bmatrix}.
\end{equation}
where the subscript $_{avg}$ means the left and right values have been averaged, $\tvec_{lr}$ is the unit vector that points from the center of the $l$\textsuperscript{th} control volume (the one to the left of the face) to the center of the $r$\textsuperscript{th} control volume (the one to the right of the face), and $l_{lr}$ is the length between these two centers. The corresponding Jacobian is approximated as follows
\begin{equation}
    \frac{\partial \Fvec^{(v)}_K}{\partial \Wvec_J} = \begin{bmatrix}
    \dfrac{\partial \Fvec^{(v)}_K(1)}{\partial \Wvec_J(1)} & \dfrac{\partial \Fvec^{(v)}_K(1)}{\partial \Wvec_J(2)} \\
    \dfrac{\partial \Fvec^{(v)}_K(2)}{\partial \Wvec_J(1)} & \dfrac{\partial \Fvec^{(v)}_K(2)}{\partial \Wvec_J(2)} \end{bmatrix},
\end{equation}
where 
\begin{equation}
    \frac{\partial \Fvec^{(v)}_K(1)}{\partial \Wvec_J(1)} =
    \left ( \mu + \sigma_k \mu_t \right)_{avg} \frac{1}{l_{lr}} \tvec_{lr} \cdot \nvec \frac{\delta_{rJ}}{\rho_r} - \left ( \mu + \sigma_k \mu_t \right)_{avg} \frac{1}{l_{lr}} \tvec_{lr} \cdot \nvec \frac{\delta_{lJ}}{\rho_l},
\end{equation}
\begin{equation}
    \frac{\partial \Fvec^{(v)}_K(2)}{\partial \Wvec_J(2)} = \left ( \mu + \sigma_\omega \mu_t \right)_{avg} \frac{1}{l_{lr}} \tvec_{lr} \cdot \nvec \frac{\delta_{rJ}}{\rho_r} - \left ( \mu + \sigma_\omega \mu_t \right)_{avg} \frac{1}{l_{lr}} \tvec_{lr} \cdot \nvec \frac{\delta_{lJ}}{\rho_l} 
\end{equation}
and $\frac{\partial \Fvec^{(v)}_K(1)}{\partial \Wvec_J(2)} = \frac{\partial \Fvec^{(v)}_K(2)}{\partial \Wvec_J(1)} = 0$.

Sources:
The discretized source terms are computed following \cref{eq:sst_source}. The Jacobian is expressed as
\begin{equation}
    \frac{\partial \Qvec_I}{\partial \Wvec_J} = \begin{bmatrix}
    \dfrac{\partial \Qvec_I(1)}{\partial \Wvec_J(1)} & \dfrac{\partial \Qvec_I(1)}{\partial \Wvec_J(2)} \\
    \dfrac{\partial \Qvec_I(2)}{\partial \Wvec_J(1)} & \dfrac{\partial \Qvec_I(2)}{\partial \Wvec_J(2)} \end{bmatrix}.
\end{equation}
According to Wilcox p.\@ 413, the elements of the matrix above are approximated as follows
\begin{equation}
    \frac{\partial \Qvec_I(1)}{\partial \Wvec_J(1)} \approx -\beta^* w_I \delta_{IJ} \qquad \frac{\partial \Qvec_I(2)}{\partial \Wvec_J(2)} \approx - 2 \beta w_I \delta_{IJ} \qquad \frac{\partial \Qvec_I(1)}{\partial \Wvec_J(2)} = \frac{\partial \Qvec_I(2)}{\partial \Wvec_J(1)} \approx 0.
\end{equation}

%---------------------------------------------------------------
\section{Implicit time integration}
%---------------------------------------------------------------
We combine \cref{eq:finite_vol_ode} for all $I$ into a single equation as follows
\begin{equation}
    \frac{d}{dt} \begin{pmatrix} \Wvec_1 \\ \Wvec_2 \\ \vdots \end{pmatrix} = \begin{pmatrix} -\frac{1}{\Omega_1} \Rvec_1 \\ -\frac{1}{\Omega_2} \Rvec_2 \\ \vdots \end{pmatrix}.
\end{equation}
Note that the above is of the same form as \cref{eq:time_stepping_ode_system}. Thus, the corresponding form of \cref{eq:backward_euler_pseudotime_stepping_system} would be
\begin{multline}
    \left [ 
    \frac{1}{\Delta t} \begin{pmatrix} \Ivec & 0 & \dots \\ 0 & \Ivec & \\ \vdots &  & \ddots \end{pmatrix} - 
    \begin{pmatrix} -\frac{1}{\Omega_1} \frac{\partial \Rvec_1(\xvec_1, \xvec_2, \dots)}{\partial \xvec_1} & -\frac{1}{\Omega_1} \frac{\partial \Rvec_1(\xvec_1, \xvec_2, \dots)}{\partial \xvec_2} & \dots \\ -\frac{1}{\Omega_2} \frac{\partial \Rvec_2(\xvec_1, \xvec_2, \dots)}{\partial \xvec_1} & -\frac{1}{\Omega_2} \frac{\partial \Rvec_2(\xvec_1, \xvec_2, \dots)}{\partial \xvec_1} & \\ \vdots & & \ddots \end{pmatrix}_{\xvec_\alpha = \Wvec_\alpha^n} \right ]
    \begin{pmatrix} \Delta \Wvec_1 \\ \Delta \Wvec_2 \\ \vdots \end{pmatrix} \\
    = \begin{pmatrix} -\frac{1}{\Omega_1} \Rvec_1(\Wvec_1^n, \Wvec_2^n,\dots) \\ -\frac{1}{\Omega_2} \Rvec_2(\Wvec_1^n, \Wvec_2^n,\dots) \\ \vdots \end{pmatrix}.
\end{multline}
Multiplying each of the major rows above one obtains
\begin{multline}
    \left [ 
    \frac{1}{\Delta t} \begin{pmatrix} \Omega_1 \Ivec & 0 & \dots \\ 0 & \Omega_2 \Ivec & \\ \vdots &  & \ddots \end{pmatrix} + 
    \begin{pmatrix} \frac{\partial \Rvec_1(\xvec_1, \xvec_2, \dots)}{\partial \xvec_1} & \frac{\partial \Rvec_1(\xvec_1, \xvec_2, \dots)}{\partial \xvec_2} & \dots \\ \frac{\partial \Rvec_2(\xvec_1, \xvec_2, \dots)}{\partial \xvec_1} & \frac{\partial \Rvec_2(\xvec_1, \xvec_2, \dots)}{\partial \xvec_2} & \\ \vdots & & \ddots \end{pmatrix}_{\xvec_\alpha = \Wvec_\alpha^n} \right ]
    \begin{pmatrix} \Delta \Wvec_1 \\ \Delta \Wvec_2 \\ \vdots \end{pmatrix} \\
    = \begin{pmatrix} -\Rvec_1(\Wvec_1^n, \Wvec_2^n,\dots) \\ -\Rvec_2(\Wvec_1^n, \Wvec_2^n,\dots) \\ \vdots \end{pmatrix}.
\end{multline}
As previously noted, the only Jacobians $\partial \Rvec_I/\partial \Wvec_J$ that are non-zero are those for which $J \in M(I)$. Thus, the second submatrix of the above is mostly sparse. The $I$th major row can be written as
\begin{equation}
\sum\limits_{J = M(I)}  \left[\frac{\Omega_I}{\Delta t}\delta_{IJ} +  \left . \frac{\partial \Rvec_I(\xvec_1, \xvec_2, \dots)}{\partial \xvec_J} \right |_{\xvec_\alpha = \Wvec_\alpha^n} \right]\Delta \Wvec_J = -\Rvec_I(\Wvec_1^n, \Wvec_2^n,\dots).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Finite Element for PDE's}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%########################################################################
\chapter{Introduction}
%########################################################################

%---------------------------------------------------------------
\section{The finite-element method}
%---------------------------------------------------------------
The finite element method is a technique for solving partial differential equations in which the solution is represented as a sum of predefined functions, which are referred to as basis functions. This is not unlike the spectral method, but the key difference is that the basis functions of the finite element method are localized, that is, they are non-zero only among some specific regions of the domain, whereas the basis functions of the spectral method are defined over the entire domain.

The finite element method typically relies on the Galerkin method to obtain a solution. This is an additional difference with the spectral method, which can be based on either the Galerkin method or other approaches such as the collocation and tau methods. For completeness, the collocation method is briefly described in \cref{sec:collocation_method}.

%---------------------------------------------------------------
\section{The Galerkin method}
%---------------------------------------------------------------
\label{sec:galerkin_method}

%--------------------------------------------
\subsection{Weak form or variational formulation}
%--------------------------------------------
Consider the problem in which a scalar $u = u(\xvec,t)$ is sought as the solution to the following
\begin{equation}
    \label{eq:fe_general_pde}
    \begin{aligned}
        &\frac{\partial u}{\partial t} + Au = f \qquad &&\text{in } \Omega \times \Rvec_+ \\
        &u = 0 && \text{on } \partial \Omega \times \Rvec_+ \\
        &u(\cdot,0) = u_0 && \text{in } \Omega 
    \end{aligned}
\end{equation}
where $A$ is any given linear operator, $\Omega$ is the spatial domain, $\partial \Omega$ its boundary, and $\Rvec_+$ the set of positive real numbers. We now introduce $\mathcal{U}$ as the space of trial functions and $\mathcal{V}$ as the space of test functions. The weak formulation of the equation above is then to find the weak solution $u\in \mathcal{U}$ such that 
\begin{equation}
    \left ( \frac{\partial u}{\partial t} , v \right ) + (Au,v) = (f,v) \qquad \forall v \in \mathcal{V},
\end{equation}
where $(a,b)= \int_\Omega av \, dV$ is the $L_2$ inner product. Using the product rule and the divergence theorem, some of the derivatives in $(Au,v)$ can be moved from operating on $u$ to operating on $v$. We label the end result as $a(u,v)$, which is a bilinear form. We also label $(f,v)$ as $L(v)$, which is a linear form. Thus, the above is rewritten as 
\begin{equation}
\label{eq:weak_form}
    \left ( \frac{\partial u}{\partial t} , v \right ) + a(u,v) = L(v) \qquad \forall v \in \mathcal{V} .
\end{equation}

Different linear operators $A$ lead to different bilinear forms. Take as an example the operator $\nabla \cdot (\lambdavec u)$, where $\lambdavec$ is a vector. For this case, the product rule and the divergence theorem lead to
\begin{align}
    \label{eq:fe_product_and_divergence}
    (\nabla \cdot (\lambdavec u), v) &= \int_\Omega \nabla \cdot (\lambdavec u) v \, dV \nonumber \\
    &= \int_\Omega \nabla \cdot (\lambdavec uv) \, dV - \int_\Omega u \lambdavec \cdot \nabla v \, dV \nonumber \\
    &= \int_{\partial \Omega} uv \lambdavec \cdot \nvec \, dS - \int_\Omega u \lambdavec \cdot \nabla v \, dV.
\end{align}
We assume that either $u$, $v$, or $\lambdavec$ is zero on $\partial \Omega$, and thus we get
\begin{equation}
    \label{eq:fe_sample_bilinear_form}
    (\nabla \cdot (\lambdavec u), v) = -(u \lambdavec, \nabla v).
\end{equation}
Some examples of operators and the corresponding bilinear forms are shown in the table below 
\begin{center}
\begin{tabular}{Sc|Sc|Sc}
    Operator & Definition & Bilinear Form \\
    \hline
    Mass & $ \lambda u $ & $(\lambda u, v)$ \\
    \hline
    Divergence & $-\nabla \cdot (\lambdavec u)$ & $(u\lambdavec, \nabla v) $ \\
    \hline
    Diffusion & $-\nabla \cdot ( \lambda \nabla u)$ & $(\lambda \nabla u, \nabla v)$ \\
    \hline
    Convection & $ \lambdavec \cdot \nabla u $ & $(\lambdavec \cdot \nabla u, v)$ \\
\end{tabular}
\end{center}

%--------------------------------------------
\subsection{Discretization into finite elements}
%--------------------------------------------
The domain $\Omega$ is now subdivided into smaller finite elements $K_j$, with $j=1,...,m$. We introduce the following function space
\begin{equation}
    \label{eq:fe_cg_discrete_space}
    \mathcal{S}_{h,k} = \{v \in C \left ( \bar{\Omega} \right ) : v(K_j) \in \Pi_{k}(K_j) \text{ for } j=1,...,m, \text{ and } v = 0 \text{ on } \partial \Omega \times \Rvec_+ \}.
\end{equation}
In the above, $\Pi_{k}(K_j)$ denotes the space of polynomials of degree $\le k$ defined within the finite element $K_j$. In other words, $\mathcal{S}_{h,k}$ is the space of piecewise-polynomial continuous functions. See for example \cref{fig:fe_cg_sample_function}. for a pictorial representation of these functions. For finite $k$, $S_{h,k}$ is a finite dimensional space: any function in $S_{h,k}$ can be constructed as a linear combination of basis polynomial functions.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../../images/cg_fe_space.pdf}
        \caption{Sample function for CG.}
        \label{fig:fe_cg_sample_function}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{../../images/dg_fe_space.pdf}
        \caption{Sample function for DG.}
        \label{fig:fe_dg_sample_function}
    \end{subfigure}
    \caption{}
    \label{fig:fe_cg_dg_sample_functions}
 \end{figure}

We now introduce $\mathcal{U}_h \subset \mathcal{U}$ and $\mathcal{V}_h \subset \mathcal{V}$ as functional spaces that belong to $\mathcal{S}_{h,k}$. The aim is to find $u_h \in \mathcal{U}_h$ such that
\begin{equation}
    \label{eq:fe_elliptic_sh_solution}
        \left ( \frac{\partial u_h}{\partial t} , v_h \right ) + a(u_h,v_h) = L(v_h) \qquad \forall v_h \in \mathcal{V}_h.
    \end{equation}
Label the finite basis of $\mathcal{U}_h$ as $\{\Phi_i\}_{i=1}^n$, where $n$ is the size of the basis (also referred to as degrees of freedom), and $\Phi_i = \Phi_i(\xvec)$. For the sake of simplicity, lets use $\mathcal{V}_h = \mathcal{U}_h$ in this description. 

Any $v_h \in \mathcal{V}_h$ can be written as $v_h = \sum_{i=1}^n V_i \Phi_i$, where $V_i = V_i(t)$. \Cref{eq:fe_elliptic_sh_solution} is then automatically satisfied if the following is satisfied
\begin{equation}
    \label{eq:fe_cg_pde_basis_requirement}
    \left ( \frac{\partial u_h}{\partial t}, \Phi_i \right ) + a(u_h, \Phi_i) = L(\Phi_i) \qquad \text{for }i=1,...,n.
\end{equation}

Since $u_h \in \mathcal{U}_h$, we can write $u_h = \sum_{j=1}^n U_j \Phi_j$, where $U_j = U_j(t)$. Thus, the above becomes
\begin{equation}
    \sum_{j=1}^n (\Phi_j, \Phi_i) \frac{dU_j}{dt} + \sum_{j=1}^n U_j a(\Phi_j,\Phi_i) = L(\Phi_i) \qquad \text{for }i=1,...,n.
\end{equation}
Define the vectors $\Uvec$ and $\bvec$, as those whose components are $U_j$ and $L(\Phi_i)$, respectively. Define the matrix $\Avec$ as that whose components are $a_{ij} = a(\Phi_j,\Phi_i)$. Similarily, define the mass matrix $\Mvec$ as that whose components are $m_{i,j} = (\Phi_j, \Phi_i)$. Then, the equation above can be written as
\begin{equation}
    \Mvec \frac{d \Uvec}{dt} + \Avec \Uvec = \bvec.
\end{equation}

%---------------------------------------------------------------
\section{The discontinuous Galerkin method}
%---------------------------------------------------------------
The dicsontinuous Galerkin (DG) method uses a space of trial functions $\mathcal{U}$ and a space of test functions $\mathcal{V}$ that consist of discontinuous functions. The discontinuities in functions lead to an alteration of the divergence theorem. In \cref{eq:fe_product_and_divergence} we used the divergence theorem, which is defined below
\begin{equation}
    \int_\Omega \nabla \cdot (\lambdavec u) \, dV = \int_{\partial \Omega} u \lambdavec \cdot \nvec \, dS.
\end{equation}
This holds true only when $u$ is a continuous function. An alternate formulation holds for cases where $u$ is discontinuous (note that we still assume $\lambdavec$ is continuous). Label $\omega_j$, with $j=1,...$ as the subdomains in $\Omega$ over which the functions are continuous. The set of all $\omega_j$ is $\tau$. Thus, we now have
\begin{equation}
    \int_\Omega \nabla \cdot (\lambdavec u) \, dV = \sum_{w_j \in \tau} \int_{w_j} \nabla \cdot (\lambdavec u) \, dV = \sum_{w_j \in \tau} \int_{\partial w_j} u \lambdavec \cdot \nvec_{w_j} \, dS,
\end{equation}
where $\nvec_{w_j}$ is the unit normal for the subdomain $w_j$. We introduce the notation 
\begin{equation}
    \llbracket u \rrbracket = u^+ - u^-,
\end{equation}
where $u^+$ is the value of $u$ on one side of the subdomain boundary and $u^-$ is the value on the other side. Additionally, introduce $\Gamma$ as the skeleton of $\tau$, that is, the inner boundaries of all the subdomains $w_j$ in the domain $\Omega_j$. We can now write
\begin{equation}
    \label{eq:dg_div_theorem_discontinuous}
    \int_\Omega \nabla \cdot (\lambdavec u) \, dV = \int_{\partial \Omega} u \lambdavec \cdot \nvec \, dS + \int_{\Gamma} \llbracket u \rrbracket \lambdavec \cdot \nvec_{\Gamma} \, dS,
\end{equation}
where $\nvec_{\Gamma}$ is a unit normal for each point in the skeleton $\Gamma$ (the orientation of $\nvec_{\Gamma}$ is arbitrary). \Cref{eq:dg_div_theorem_discontinuous} is the divergence theorem for discontinuous functions $u$. We introduce the notation
\begin{equation}
\{ u \} = \frac{1}{2} \left ( u^+ + u^- \right ).
\end{equation}
From its definitions, it can be shown that
\begin{equation}
    \llbracket uv \rrbracket = \{ u \} \llbracket v \rrbracket + \llbracket u \rrbracket \{ v \}.
\end{equation}
Thus, the divergence theorem for two discontinuous functions $u$ and $v$ can be written as
\begin{equation}
    \label{eq:div_theorem_2funcs}
    \int_\Omega \nabla \cdot (\lambdavec uv) \, d\xvec = \int_{\partial \Omega} uv \lambdavec \cdot \nvec \, dS + \int_{\Gamma} \left ( \{ u \} \llbracket v \rrbracket + \llbracket u \rrbracket \{ v \} \right )\lambdavec \cdot \nvec_{\nvec_\Gamma} \, dS.
\end{equation}

Consider the derivation carried out to obtain \cref{eq:fe_sample_bilinear_form}. If both $u$ and $v$ are discontinuous, then \cref{eq:div_theorem_2funcs} is used rather than the traditional divergence theorem and the result becomes
\begin{equation}
    \label{eq:from_strong_to_weak_form}
    (\nabla \cdot(\boldsymbol{\lambda}u),v) = -(u\boldsymbol{\lambda},\nabla v) + \int_{\Gamma} \left ( \{u\} \llbracket v \rrbracket + \llbracket u \rrbracket \{v\} \right ) \lambdavec \cdot \nvec_{\Gamma} \, dS.
\end{equation}
Thus, for DG, an additional term appears for every bilinear form. For example, \cref{eq:weak_form} for the weak formulation given the specific operator $\nabla \cdot (\boldsymbol{\lambda} u)$ is now
\begin{equation}
        \left ( \frac{\partial u}{\partial t} , v \right ) + a(u,v)  + \int_{\Gamma} \left ( \{ u \} \llbracket v \rrbracket + \llbracket u \rrbracket \{ v \} \right ) \lambdavec \cdot \nvec_{\Gamma} \, dS = L(v) \qquad \forall v \in \mathcal{V} .
\end{equation}

Unlike the finite dimensional space shown in \cref{eq:fe_cg_discrete_space}, we now use the following for DG 
\begin{equation}
    \label{eq:dg_discretized_fe_space}
    \mathcal{S}_{h,k} = \{v \in L_2 \left ( \bar{\Omega} \right ): v(K_j) \in \Pi_{k}(K_j) \text{ for } j=1,...,m \},
\end{equation}
where $\Pi_{k}(K_j)$ can be the same space as in the continuous Galerkin case. The main difference, however, is that the functions $v \in S_{h,k}$ can now be discontinuous from element to element (they do not belong to $C(\bar{\Omega})$). See \cref{fig:fe_dg_sample_function} for a pictorial representation of these functions. We note that the functions are discontinuous only across elements, and not within elements. Thus, we can now make the correspondence $\omega_j \to K_j$. The set of all $K_j$ is $\tau_h$, and the skeleton is $\Gamma_h$. The corresponding form of \cref{eq:fe_elliptic_sh_solution} is thus
\begin{equation}
    \label{eq:dg_main_eq_total}
    \left ( \frac{\partial u_h}{\partial t} , v_h \right ) + a(u_h,v_h)  + \int_{\Gamma_h} \left ( \{ u_h \} \llbracket v_h \rrbracket + \llbracket u_h \rrbracket \{ v_h \} \right )\lambdavec \cdot \nvec_{\Gamma_h} \, dS = L(v_h) \qquad \forall v_h \in \mathcal{V}_h .
\end{equation}
Note that we can break up the integrals above into sums of integrals over each finite element. That is,
\begin{equation}
    \sum_{K_j \in \tau_h} \left ( \frac{\partial u_h}{\partial t} , v_h \right )_{K_j} + \sum_{K_j \in \tau_h} a(u_h,v_h)_{K_j}  + \sum_{K_j \in \tau_h} \int_{\partial K_j} u_h v_h \lambdavec \cdot \nvec_{K_j} \, dS = \sum_{K_j \in \tau_h} L(v_h)_{K_j} \qquad \forall v \in \mathcal{V} .
\end{equation}
To satisfy the above, we can require
\begin{equation}
    \label{eq:dg_main_eq_element_wise}
    \left ( \frac{\partial u_h}{\partial t} , v_h \right )_{K_j} + a(u_h,v_h)_{K_j}  + \int_{\partial K_j} u_h v_h \lambdavec \cdot \nvec_{K_j} \, dS = L(v_h)_{K_j} \qquad \forall v_h \in \mathcal{V}_h .
\end{equation}

For many schemes, the flux term $f(u_h) = \boldsymbol{\lambda} u_h$ in the surface integrals of \cref{eq:dg_main_eq_total,eq:dg_main_eq_element_wise} is replaced by a numerical flux $\hat{f}(u_h)$. In this case, we'll focus on the numerical flux $\hat{f}(u_h) = \boldsymbol{\lambda} \hat{u}_h$, where 
\begin{equation}
    \label{eq:dg_numerical_flux}
    \hat{u}_h = \{ u_h \} + \delta \frac{\left | \boldsymbol{\lambda} \cdot \nvec_{\Gamma_h} \right |}{\boldsymbol{\lambda} \cdot \nvec_{\Gamma_h}} \llbracket u_h \rrbracket.
\end{equation}
Since $\hat{u}_h$ is now continuous, we have $\llbracket \hat{u}_h \rrbracket = 0$ and $\{ \hat{u}_h \} = \hat{u}_h$. Thus, \cref{eq:dg_main_eq_total} becomes
\begin{equation}
    \left ( \frac{\partial u_h}{\partial t} , v_h \right ) + a(u_h,v_h)  + \int_{\Gamma_h} \hat{u}_h \llbracket v_h \rrbracket \lambdavec \cdot \nvec_{\Gamma_h} \, dS = L(v_h) \qquad \forall v_h \in \mathcal{V}_h,
\end{equation}
or, after expanding the definition of $\hat{u}_h$
\begin{equation}
    \label{eq:dg_main_eq_total_with_upwinding_weak}
    \left ( \frac{\partial u_h}{\partial t} , v_h \right ) + a(u_h,v_h)  + \int_{\Gamma_h} \lambdavec \cdot \nvec_{\Gamma_h} \{ u_h \} \llbracket v_h \rrbracket \, dS + \delta \int_{\Gamma_h} \left | \boldsymbol{\lambda} \cdot \nvec_{\Gamma_h} \right | \llbracket u_h \rrbracket \llbracket v_h \rrbracket \, dS = L(v_h) \qquad \forall v_h \in \mathcal{V}_h.
\end{equation}

Given that $a(u_h, v_h) = -(u_h \boldsymbol{\lambda}, \nabla v_h)$ for the operator $\nabla \cdot (\boldsymbol{\lambda} u)$, we can rewrite \cref{eq:dg_main_eq_total_with_upwinding_weak} as
\begin{equation}
    \left ( \frac{\partial u_h}{\partial t} , v_h \right ) - (u_h \boldsymbol{\lambda}, \nabla v_h) + \int_{\Gamma_h} \lambdavec \cdot \nvec_{\Gamma_h} \{ u_h \} \llbracket v_h \rrbracket \, dS + \delta \int_{\Gamma_h} \left | \boldsymbol{\lambda} \cdot \nvec_{\Gamma_h} \right | \llbracket u_h \rrbracket \llbracket v_h \rrbracket \, dS = L(v_h) \qquad \forall v_h \in \mathcal{V}_h.
\end{equation}
Using \cref{eq:from_strong_to_weak_form}, we get
\begin{multline}
    \left ( \frac{\partial u_h}{\partial t} , v_h \right ) + (\nabla \cdot ( \boldsymbol{\lambda} u_h ), v_h ) -\int_{\Gamma_h} \left ( \{ u_h \} \llbracket v_h \rrbracket + \llbracket u_h \rrbracket \{ v_h \} \right ) \boldsymbol{\lambda} \cdot \nvec_{\Gamma_h} \, dS\\
    + \int_{\Gamma_h} \lambdavec \cdot \nvec_{\Gamma_h} \{ u_h \} \llbracket v_h \rrbracket \, dS + \delta \int_{\Gamma_h} \left | \boldsymbol{\lambda} \cdot \nvec_{\Gamma_h} \right | \llbracket u_h \rrbracket \llbracket v_h \rrbracket \, dS = L(v_h) \qquad \forall v_h \in \mathcal{V}_h,
\end{multline}
which, upon simplifying gives
\begin{equation}
    \label{eq:dg_main_eq_total_with_upwinding_strong}
    \left ( \frac{\partial u_h}{\partial t} , v_h \right ) + (\nabla \cdot ( \boldsymbol{\lambda} u_h ), v_h ) -\int_{\Gamma_h} \boldsymbol{\lambda} \cdot \nvec_{\Gamma_h}\llbracket u_h \rrbracket \{ v_h \} \, dS + \delta \int_{\Gamma_h} \left | \boldsymbol{\lambda} \cdot \nvec_{\Gamma_h} \right | \llbracket u_h \rrbracket \llbracket v_h \rrbracket \, dS = L(v_h) \qquad \forall v_h \in \mathcal{V}_h.
\end{equation}
\Cref{eq:dg_main_eq_total_with_upwinding_weak} is the weak form of the equation, whereas \cref{eq:dg_main_eq_total_with_upwinding_strong} is the strong form.

%---------------------------------------------------------------
\section{Multi-dimensional finite elements}
%---------------------------------------------------------------
To describe the application of finite elements to cases where the unknowns are vectors (e.g. $\uvec = [u_x,u_y,u_z]$), we'll use the following elliptic PDE
\begin{equation}
    \label{eq:fe_elliptic_pde_vector}
    \sum_{\beta=x,y,z} -\frac{\partial}{\partial x_\beta} \left ( \frac{\partial u_\alpha}{\partial x_\beta} + \frac{\partial u_\beta}{\partial x_\alpha} \right ) = f_\alpha \qquad \text{in } \Omega^3 \text{ for } \alpha = x,y,z.
\end{equation}
For the above, $\uvec = 0$ at boundaries. As for the 1D case, we introduce trial and test function spaces, but these are now three dimensional and are labeled as $\mathcal{U}^3$ and $\mathcal{V}^3$. As for the 1D case again, we'll assume for simplicity that $\mathcal{U}^3$ and $\mathcal{V}^3$ are the same function space. The objective is now to find the weak solution $\uvec \in \mathcal{U}^3$ such that
\begin{equation}
    \int_{\Omega^3} \sum_{\alpha=x,y,z} \sum_{\beta=x,y,z} -\frac{\partial}{\partial x_\beta} \left ( \frac{\partial u_\alpha}{\partial x_\beta} + \frac{\partial u_\beta}{\partial x_\alpha} \right ) v_\alpha \, dV = \int_{\Omega^3} \sum_{\alpha=x,y,z} f_\alpha v_\alpha \,dV \qquad \forall \vvec \in \mathcal{V}^3.
\end{equation}
We now use the product rule and divergence theorem to obtain
\begin{equation}
    \int_{\Omega^3} \sum_{\alpha=x,y,z} \sum_{\beta=x,y,z} \left ( \frac{\partial u_\alpha}{\partial x_\beta} + \frac{\partial u_\beta}{\partial x_\alpha} \right ) \frac{\partial v_\alpha}{\partial x_\beta} \, dV = \int_{\Omega^3} \sum_{\alpha=x,y,z} f_\alpha v_\alpha \,dV \qquad \forall \vvec \in \mathcal{V}^3.
\end{equation}
As before, we introduce the discrete function spaces $\mathcal{U}_h^3 \subset \mathcal{U}^3$ and $\mathcal{V}^3_h \subset \mathcal{V}^3$ that belong to $\mathcal{S}_{h,k}^3$, which will be defined at a later point. The aim is now to find $\uvec_h \in \mathcal{U}_h^3$ such that
\begin{equation}
    \int_{\Omega^3} \sum_{\alpha=x,y,z} \sum_{\beta=x,y,z} \left ( \frac{\partial u_{h,\alpha}}{\partial x_\beta} + \frac{\partial u_{h,\beta}}{\partial x_\alpha} \right ) \frac{\partial v_{h,\alpha}}{\partial x_\beta} \, dV = \int_{\Omega^3} \sum_{\alpha=x,y,z} f_\alpha v_{h,\alpha} \,dV \qquad \forall \vvec_h \in \mathcal{V}_h^3.
\end{equation}
We expand $\vvec_h$ as follows
\begin{equation}
    \vvec_h = \sum_i^n \Vvec_i \Phi_i,
\end{equation}
where $\Vvec_i=[V_{i,x}, V_{i,y}, V_{i,z}]$ is now a vector but $\Phi_i$ is still a scalar. We thus have
\begin{multline}
    \sum_{\alpha=x,y,z} \sum_i^n V_{i,\alpha} \int_{\Omega^3} \sum_{\beta=x,y,z} \left ( \frac{\partial u_{h,\alpha}}{\partial x_\beta} + \frac{\partial u_{h,\beta}}{\partial x_\alpha} \right ) \frac{\partial \Phi_i}{\partial x_\beta} \, dV = \\
    \sum_{\alpha=x,y,z} \sum_i^n V_{i,\alpha} \int_{\Omega^3} f_\alpha \Phi_i \,dV \qquad \forall \vvec_h \in \mathcal{V}_h^3.
\end{multline}
The above is satisfied if we require that the following holds
\begin{equation}
    \label{eq:fe_3d_pde_basis_requirement}
    \int_{\Omega^3} \sum_{\beta=x,y,z} \left ( \frac{\partial u_{h,\alpha}}{\partial x_\beta} + \frac{\partial u_{h,\beta}}{\partial x_\alpha} \right ) \frac{\partial \Phi_i}{\partial x_\beta} \, dV = \int_{\Omega^3} f_\alpha \Phi_i \,dV \qquad \text{for } i = 1,...,n \text{ and } \alpha = x,y,z.
\end{equation}
The equation above would be the analogue of \cref{eq:fe_cg_pde_basis_requirement} for 1D. That is, it is not necessary to test the PDE against every function in $\mathcal{V}_h^3$, but only against the basis functions of that space. We'll re-group the result as follows
\begin{multline}
    \int_{\Omega^3} \sum_{\beta=x,y,z} \frac{\partial u_{h,\alpha}}{\partial x_\beta} \frac{\partial \Phi_i}{\partial x_\beta} \,dV + \int_{\Omega^3} \sum_{\beta=x,y,z} \frac{\partial u_{h,\beta}}{\partial x_\alpha} \frac{\partial \Phi_i}{\partial x_\beta} \, dV \\
    = \int_{\Omega^3} f_\alpha \Phi_i \,dV \qquad \text{for } i = 1,...,n \text{ and } \alpha = x,y,z.
\end{multline}
The next step is to introduce 
\begin{equation}
    \uvec_h = \sum_j^n \Uvec_j \Phi_j,
\end{equation}
where again $\Uvec_j = [U_{i,x}, U_{i,y}, U_{i,z}]$. Plugging this in the PDE we have
\begin{multline}
    \sum_j^n U_{j,\alpha} \int_{\Omega^3} \sum_{\beta=x,y,z} \frac{\partial \Phi_j}{\partial x_\beta} \frac{\partial \Phi_i}{\partial x_\beta} \,dV + \sum_{\beta=x,y,z} \sum_j^n U_{j,\beta} \int_{\Omega^3} \frac{\partial \Phi_j}{\partial x_\alpha} \frac{\partial \Phi_i}{\partial x_\beta} \, dV \\
    = \int_{\Omega^3} f_\alpha \Phi_i \,dV \qquad \text{for } i = 1,...,n \text{ and } \alpha = x,y,z.
\end{multline}
If we now introduce the bilinear forms 
\begin{equation}
    a_{ij} = \int_{\Omega^3} \sum_{\beta=x,y,z} \frac{\partial \Phi_j}{\partial x_\beta} \frac{\partial \Phi_i}{\partial x_\beta} \,dV,
\end{equation}
\begin{equation}
    b_{ij}^{\alpha \beta} = \int_{\Omega^3}\frac{\partial \Phi_j}{\partial x_\alpha} \frac{\partial \Phi_i}{\partial x_\beta} \, dV,
\end{equation}
and
\begin{equation}
    L_i^\alpha = \int_{\Omega^3} f_\alpha \Phi_i \,dV,
\end{equation}
we can write the linear system of equations as
\begin{equation}
    \sum_j^n U_{j,\alpha} a_{ij} + \sum_{\beta=x,y,z} \sum_j^n U_{j,\beta} b_{ij}^{\alpha \beta} = L_i^\alpha \qquad \text{for } i = 1,...,n \text{ and } \alpha = x,y,z.
\end{equation}

%---------------------------------------------------------------
\section{The Petrov Galerkin method}
%---------------------------------------------------------------

%########################################################################
\chapter{Elliptic}
%########################################################################

%########################################################################
\chapter{Parabolic}
%########################################################################

%########################################################################
\chapter{Hyperbolic}
%########################################################################

\appendix

%########################################################################
\chapter{Notes on functional analysis}
%########################################################################

%---------------------------------------------------------------
\section{Some terminology}
%---------------------------------------------------------------
\begin{itemize}
    \item Cauchy sequence: a sequence $v_1$, $v_2$, $v_3$, ... \@is a Cauchy sequence if for every possitive real number $\epsilon$, there is a possitive integer $N$ such that for all possitive integers $m,n>N$, $||v_m-v_n||<\epsilon$. 
    \item Complete inner produce space: an inner product space $\mathcal{V}$ is complete if every Cauchy sequence $\{v_i\}_{i=1}^\infty$ in $\mathcal{V}$ has a limit $v = \lim v_i \in \mathcal{V}$.
    \item Compact set: a set is compact if it is bounded and closed.
    \item Coercive bilinear form: a bilinear form $a(:,:)$ is coercive in a Hilbert space $\mathcal{V}$ if 
    \begin{equation}
        a(v,v) \ge \alpha ||v||_\mathcal{V}^2, \qquad \forall v \in \mathcal{V}, \qquad \text{with }\alpha > 0.
    \end{equation}
    \item Bounded linear form: a linear form is bounded in the normed vector space $\mathcal{V}$ if there exists an $M>0$ such that $|L(v)| \le M ||v||$, for every $v \in \mathcal{V}$. 
    \item Bounded bilinear form: a bilinear form is bounded in the normed vector space $\mathcal{V}$ if there exists and $M>0$ such that $|a(w,v)| \le M ||w||\;||v||$, for every $w,v \in \mathcal{V}$.
\end{itemize}

%---------------------------------------------------------------
\section{Useful equalities and inequalities}
%---------------------------------------------------------------
\begin{align}
    &|ab| = |a||b| \qquad \forall a,b \in \mathbb{C} \\
    &|a + b| \le |a| + |b| \qquad \forall a,b \in \mathbb{C} \\
    &|(w,v)| \le ||w||\;||v|| \qquad \forall v,w \in \text{Inner product space (Cauchy-Schwarz inequality)} \\
    &||w + v|| \le ||w|| + ||v|| \qquad \forall v,w \in \text{Normed space (triangle inequality)} \\
    &\left | \int_a^b v(x) \, dx \right| \le \int_a^b |v(x)| \, dx \\
    &\left \| \int_a^b v(x,y) \, dx \right \| \le \int_a^b ||v(x,y)|| \, dx \text{ where the norm is over the y-domain}.
\end{align}

%---------------------------------------------------------------
\section{The weak derivative}
%---------------------------------------------------------------
If $v \in C^1(\bar{\Omega})$, then through integration by parts
\begin{equation}
    \int_\Omega \frac{\partial v}{\partial x_i} \phi \, dV = -\int_\Omega v \frac{\partial \phi}{\partial x_i} \, dV \qquad \forall \phi \in C_0^1(\Omega).
\end{equation}
However, if $v \in L_2(\Omega)$ but not necessarily in $C^1(\bar{\Omega})$, we cannot write the equation above. Instead, we ask, is there a function $w$ such that the following holds?
\begin{equation}
     \int_\Omega w \phi \, dV = -\int_\Omega v \frac{\partial \phi}{\partial x_i} \, dV \qquad \forall \phi \in C_0^1(\Omega).
\end{equation}
This can be rewritten as $(w,\phi) = L(\phi)$, where $L(\phi) = -\int_\Omega v \frac{\partial \phi}{\partial x_i} dV$. If $L(\phi)$ is bounded in $L_2$, Riesz' representation theorem then states a unique solution $w \in L_2(\Omega)$ exists. This $w$ is the weak derivative.

More generally, if $v \in C^k(\bar{\Omega})$, then through integration by parts
\begin{equation}
    \int_\Omega D^\alpha v \phi \, dV = (-1)^{|\alpha|} \int_\Omega v D^\alpha \phi \, dV \qquad \forall |\alpha| \le k,\, \forall \phi \in C_0^{|\alpha|}(\Omega).
\end{equation}
However, if $v \in L_2(\Omega)$ but not necessarily in $C^k(\bar{\Omega})$, we cannot write the equation above. Instead, we ask, is there a function $w$ such that the following holds?
\begin{equation}
    \int_\Omega w \phi \, dV = (-1)^{|\alpha|} \int_\Omega v D^\alpha \phi \, dV \qquad \forall \phi \in C_0^{|\alpha|}(\Omega).
\end{equation}
As before, if the left-hand-side operator is bounded, then we have a unique solution $w \in L_2(\Omega)$. This $w$ is the weak derivative. Often, weak derivatives are referred to as ``$D^\alpha v$ in the weak sense.''

%---------------------------------------------------------------
\section{Function spaces}
%---------------------------------------------------------------
We label function spaces using \textbackslash mathcal notation, except for the three main function spaces for continuous, square-integrable, and Sobolev functions. 
\begin{itemize}
    \item $C^0$: the set of all continuous functions
    \item $C^k$ the set of all functions whose derivatives up to order $k$ all exist and are continuous. These are called continuously differentiable functions of order $k$.
    \item $L_2$: the set of all functions that are square integrable.
    \item $H^k$: the set of all $L_2$ functions whose weak partial derivatives up to order $k$ also belong to $L_2$. 
    \item Let $\Omega$ be a bounded domain in $\mathbb{R}^d$ with smooth or polygonal boundary. Then part of the Sobolev embedding theorem can be written as
    \begin{equation}
        H^k(\Omega) \subset C^l(\bar{\Omega}) \text{ if } k > l + d/2.
    \end{equation}
    Thus, we have $H^m(\Omega) \subset C^{m-1}(\bar{\Omega})$ for $\Omega \in \mathbb{R}$ and $H^m(\Omega) \subset C^{m-2}(\bar{\Omega})$ for $\Omega \in \mathbb{R}^2$ or $\mathbb{R}^3$.
\end{itemize}

\begin{figure}[ht]
   \centering
   \includegraphics[width=0.8\textwidth]{../../images/function_spaces.png}
   \caption{Map of function spaces.}
   \label{fig:function_spaces}
\end{figure}

\setlength{\cellspacetoplimit}{3pt}
\setlength{\cellspacebottomlimit}{3pt}

\begin{center}
\begin{tabular}{Sc|Sc|Sc}
    Space & Norm & Inner product \\
    \hline
    $C(M)$ & $ \displaystyle ||v||_C = \sup_{x \in M} |v(x)| $ & X \\
    \hline
    $C^k(M)$ & $\begin{aligned} &||v||_{C^k} = \max_{|\alpha| \le k} ||D^\alpha v||_C \\ &|v|_{C^k}=\max_{|\alpha| = k} ||D^\alpha v||_C \end{aligned}$ & X \\
    \hline
    $L_p(\Omega)$ & $ \displaystyle ||v||_{L_p} = \left ( \int_\Omega |v|^p \, dV \right)^{1/p} $ & X \\
    \hline
    $L_2(\Omega)$ & $ \displaystyle ||v||_{L_2} = \left ( \int_\Omega |v|^2 \, dV \right)^{1/2} $ & $ \displaystyle (v,w) = \int_\Omega vw^* \, dV $ \\
    \hline
    $H^k(\Omega)$ & $ \begin{aligned} &||v||_k = \left ( \sum_{|\alpha| \le k} ||D^\alpha v||^2 \right )^{1/2} \\ &|v|_k = \left ( \sum_{|\alpha| = k} ||D^\alpha v||^2 \right )^{1/2} \end{aligned}$ & $ \displaystyle (v,w)_k = \sum_{|\alpha \le k|} (D^\alpha v, D^\alpha w) $
\end{tabular}
\end{center}

%########################################################################
\chapter{The collocation method}
%########################################################################
\label{sec:collocation_method}
Finite-element methods rely on the Galerkin method. Spectral methods can rely on the Galerkin method, but also on others such as the collocation and tau method. In this appendix we describe the collocation method.

As with the Galerkin method, we aim to solve \cref{eq:fe_general_pde} by assuming the solution is $u_h \in \mathcal{U}_h$. This assumption allows us to write $u_h= \sum_{j=1}^n U_j \Phi_j$ and thus the governing PDE can be written as
\begin{equation}
    \sum_{j=1}^n \frac{dU_j}{dt} \Phi_j + \sum_{j=1}^n U_j A\Phi_j = f.
\end{equation}
The above equation is evaluated at a discrete set of $n$ points, termed the collocation points. Thus we have
\begin{equation}
    \sum_{j=1}^n \frac{dU_j}{dt} \Phi_j(x_i) + \sum_{j=1}^n U_j (A\Phi_j)(x_i) = f(x_i) \qquad \text{for }i=1,...,n.
\end{equation}
The above can be written as
\begin{equation}
    \tilde{\Mvec} \frac{d \Uvec}{dt} + \tilde{\Avec} \Uvec = \tilde{\bvec},
\end{equation}
where $\tilde{\Mvec}$ is a matrix whose components are $\tilde{m}_{ij} = \Phi_j(x_i)$, $\tilde{\Avec}$ is a matrix whose components are $a_{ij} = (A\Phi_j)(x_i)$, and $\tilde{\bvec}$ is a vector whose components are $f(x_i)$.

\end{document}