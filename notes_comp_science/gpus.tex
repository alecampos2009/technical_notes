\documentclass[a4paper,11pt]{article}

\usepackage{fullpage}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hhline}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstset { %
    language=C++,
    basicstyle=\footnotesize,% basic font setting
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\title{Basics of GPUs}
\author{Alejandro Campos}

\begin{document}
\maketitle
\tableofcontents

%------------------------------------------------------------------------
\section{Introduction}
%------------------------------------------------------------------------
There are two key terms used when dealing with heterogeneous architectures, namely, the ``host'' and the ``device''. The host refers to the CPU available on the system, and the device to the GPU. 

In these notes we'll rely on MFEM to handle algorithms on the GPU. For example, MFEM will be used to move data between host and device (Sec.\@ \ref{sec:memory_management}) and to execute for-loops on the GPU (Sec.\@ \ref{sec:for_loops}).

%------------------------------------------------------------------------
\section{Memory management}
%------------------------------------------------------------------------
\label{sec:memory_management}
Host memory resides in the CPU, and device memory in the GPU. In many systems, memory allocated on the host can only be accessed by the CPU, and memory allocated on the device can only be accessed by the GPU. Thus, if you have an array that lives in host memory and want to alter its entries on the GPU, you first need to transfer it to device memory.

This can be performed using MFEM's memory manager. Below are three ways we can transfer data from the host to the device.
\begin{lstlisting}
    mfem::Vector rho(...);
    // you can now use the vector on the CPU, e.g. you can initialize its entries, etc.

    double* d_rho = rho.Read();
    // the pointer d_rho can now be used on the GPU, only to read the entries of the rho vector
    // E.g.
    // double val = d_rho[nth_entry];

    double* d_rho = rho.Write();
    // the pointer d_rho can now be used on the GPU, only to write to the entries of the rho vector
    // E.g.
    // d_rho[nth_entry] = 1.234;
    // note: if you try to read one of the entries on the GPU, you might get an unexpected value

    double* d_rho = rho.ReadWrite();
    // the pointer d_rho can now be used on the GPU, to either read or write to the entries of the rho vector
\end{lstlisting}

If the data is currently residing on device memory, one can bring it back to host memory by using the analogues of the above. That is,
\begin{lstlisting}
    rho.HostRead();

    rho.HostWrite();

    rho.HostReadWrite();
\end{lstlisting}
 The \texttt{rho} object can then be used on the CPU again as one would normally do. On the other hand, one can also generate explicit pointers to access data on the host, in a similar manner to the pointers for device data:
\begin{lstlisting}
    double* h_rho = rho.HostRead();

    double* h_rho = rho.HostWrite();

    double* h_rho = rho.HostReadWrite();
\end{lstlisting}

MFEM objects (array, vector, dense matrix, dense tensor) have two member variables used to determine its state, these are \texttt{VALID\_HOST} and \texttt{VALID\_DEVICE}. When one first creates an object, MFEM will set \texttt{VALID\_HOST} to true and \texttt{VALID\_DEVICE} to false. After that, if you call any of the memory-movement functions (e.g. \texttt{Read()}, \texttt{HostWrite()}), then \texttt{VALID\_HOST} and \texttt{VALID\_DEVICE} would be updates as shown on the table below.
\begin{center}
    \begin{tabular}{c | c | c}
        %\hline
        & \texttt{VALID\_HOST} & \texttt{VALID\_DEVICE} \\
        \hhline{=|=|=}
        \texttt{Read()} & does nothing & true \\ 
        %\hline
        \texttt{Write()} & false & true \\
        %\hline
        \texttt{ReadWrite()} & false & true \\
        %\hline
        \texttt{HostRead()} & true & does nothing \\ 
        %\hline
        \texttt{HostWrite()} & true & false \\
        %\hline
        \texttt{HostReadWrite()} & true & false \\
        %\hline
    \end{tabular}
\end{center}
Let's now consider a few specific cases:
\begin{itemize}
    \item You create an object, then call \texttt{Write()} or \texttt{ReadWrite()} to modify its value on the device. At this point \texttt{VALID\_HOST} would be false and \texttt{VALID\_DEVICE} would be true. You now read this data on the host but forget beforehand to do \texttt{HostRead()} or a \texttt{HostReadWrite()}. To catch this mistake, you can compile MFEM in debug mode and then run the code. This will lead to a runtime error and a stack trace since the host is now possibly out-of-sync with the device. 
    
    \item As an example, consider the object \texttt{mfem::Vector<double> rho}. After a call to \texttt{Read()} \texttt{VALID\_DEVICE} would be true and \texttt{VALID\_HOST} would still also be true. Let's say that on the host you now want to only read the entries of \texttt{rho} using the \texttt{()} operator, e.g\@ \texttt{double new\_var = 2 + rho[i]}. This is perfectly reasonable since the device has not modified the entries of the object. However, when using the parenthesis operator MFEM doesn't automatically know that you are going to only read this entry, as opposed to modifying it as well, e.g.\@ \texttt{rho(i)=new\_value}. If you were to modify it, its value on the device would no longer be valid and hence the status of the flag \texttt{VALID\_DEVICE}, which was left as true by the \texttt{()} operator, is no longer correct. MFEM thus provides two versions of the \texttt{()} operator, one that is used for reading entries only and another that is used for reading and modifying entries. The one used for reading entries can be used when \texttt{VALID\_HOST} and \texttt{VALID\_DEVICE} are both true, whereas the one for modifying entries requires the object to previously have \texttt{VALID\_HOST} set to true and \texttt{VALID\_DEVICE} to false. To use the former, you use the \texttt{mfem::AsConst()} function, as in \texttt{mfem::AsConst(rho)(i)}. To use the latter, you just use the \texttt{()} operator as usual.
\end{itemize}    

%------------------------------------------------------------------------
\section{Kernel execution}
%------------------------------------------------------------------------
\label{sec:for_loops}
A kernel is a basic unit of code to be executed on the GPU (e.g. a loop, function, or program). In this section we'll focus on kernels that consist of for-loops. To execute a for-loop on the GPU, one invokes one of the various macros available in MFEM. The standard for-loop macro is shown below:
\begin{lstlisting}
    mfem::forall(N, [=] MFEM_HOST_DEVICE (int i) \{\dots \} )
\end{lstlisting}
The second argument above, i.e. \texttt{[=] MFEM\_HOST\_DEVICE (int i) \{ \dots \}}, is a lambda expression, which is essentially a small function without a name. The body of the lambda expression, i.e. the code to be executed, is depicted by the dots \dots. The first argument of the lambda expression, i.e. \texttt{[=]}, is the capture clause. It is used to indicate how variables in the enclosing scope should be used, or ``captured'', within the for loop. For example
\begin{itemize}
    \item \texttt{[=]} capture variables by value
    \item \texttt{[\&]} capture variables by reference
    \item \texttt{[]} do not access any variables in the enclosing scope
\end{itemize}
The second argument, \texttt{MFEM\_HOST\_DEVICE}, allows the lambda expression to be called from both the host and the device. The third argument in the lambda expression is the parameter list. These are essentially inputs to the lambda expression that can be used within its body. 

An specific example on how to use \texttt{mfem::forall} is as follows. Imagine you have the following standard for-loop code
\begin{lstlisting}
    mfem::Vector rho(...);

    for (int i = 0; i < rho.Size(); i++)
    {
        rho[i] = 1.234;
    }
\end{lstlisting}
Re-writing the above using the mfem macro would look like this
\begin{lstlisting}
    mfem::Vector rho(...);

    double *d_rho = rho.Write()

    mfem::forall(rho.Size(), [=] MFEM_HOST_DEVICE (int i)
    {
        d_rho[i] = 1.234;
    });
\end{lstlisting}
For the above, the GPU will launch multiple thread blocks, which are collections of threads. Each thread in a block takes care of a single instance of the for loop. That is, one thread will execute the contents within the for-loop for value \texttt{i=0}, a second thread will execute the same code for \texttt{i=1} and so on till one last thread takes care of \texttt{i=rho.Size()-1}.

Since lambda expressions can capture variables from the enclosing scope only, a lambda expression defined within a member function of a class cannot capture a member variable of that same class. For this scenario, one needs to use a local version of that variable, as shown below
\begin{lstlisting}
    mfem::Vector rho(...);

    const double factor = 10.0;
    const double rho_min = rho_min_; // here rho_min_ is a member variable of a class

    double *d_rho = rho.Write()

    mfem::forall(rho.Size(), [=] MFEM_HOST_DEVICE (int i) 
    {
        d_rho[i] = factor * density_min;
    });
\end{lstlisting}
In the example above, if we had tried to use \texttt{rho\_min\_} directly the device lambda would have captured and tried to dereference the \texttt{this} pointer (\texttt{this->rho\_min\_}), which is non-portable (or would result in extra memory movement if it was portable). We note that with nvcc (the NVIDIA compiler) you can compile a device kernel that uses \texttt{rho\_min\_} directly, but if you run with \texttt{--atsdisable} it will segfault. With ROCm+hip (one of the AMD compilers) you will get a compilation error.

Another key difference between standard for-loops and \texttt{mfem::forall} is the way in which one exits the loop. For example, consider the following standard for-loop in which specific iterations are skipped if a condition is met
\begin{lstlisting}
    for (int i = 0; i < N; ++i)
    {
        // skip to next iteration if condition is met
        if (condition to be met)
        {
            continue;
        }
        ...
    }
\end{lstlisting}
The equivalent for a GPU kernel would be
\begin{lstlisting}
    mfem::forall(N, [=] MFEM_HOST_DEVICE (int i) 
    {
        // skip to next iteration if condition is met
        if (condition to be met)
        {
            return;
        }
        ...
    });
\end{lstlisting}
That is, we use \texttt{return} rather than \texttt{continue}. The reason for this is that the GPU version of code is essentially a for-loop that calls lambda expressions, i.e.
\begin{lstlisting}
    // mfem::forall(N, lambda ) :=
    for (int iter = 0; iter < N: iter++)
    {
        lambda(iter);
    }
\end{lstlisting}
Thus, what you want to do is ``return'' from \texttt{lambda(iter)} so that the \texttt{iter} iterator can go on to its next value. 

%------------------------------------------------------------------------
\section{Memory spaces}
%------------------------------------------------------------------------
\begin{itemize}
    \item Global: GPU memory that is typically dynamically allocated (with malloc/free or something similar) and is accessible by all threads, from all thread blocks. This means that a thread can read and write any value in global memory.
    \item Local: GPU memory that is typically statically allocated from within a kernel. It is only visible, and therefore accessible, by the thread allocating it. All threads executing a kernel will have their own privately allocated local memory.
    \item Unified/Managed: unified memory (same as managed memory) is a single global memory buffer that can be accessed and used in multiple memory spaces (e.g. host and device). The memory will automatically migrate or stay in sync if it is not where it needs to be. Unified memory is allocated with special routines, e.g. \texttt{cudaMallocManaged()} or \texttt{hipMallocManaged()}. It can also be a lower type of memory because the memory copy won't happen until one of the spaces ``faults'' on the memory address.
    \item Temporary
    \item Shared
\end{itemize}

%------------------------------------------------------------------------
\section{Hierarchical parallelism}
%------------------------------------------------------------------------
Let's say one has loops within loops, and wants to execute all of them on the GPU. One way to do so is as following
\begin{lstlisting}
    Vector rho;
    rho.UseDevice(true);

    const int size_1 = ...;
    const int size_2 = ...;
    const int size_3 = ...;
    const int size_4 = ...;

    auto d_rho = Reshape(rho.Write(), size_1, size_2, size_3, size_4);

    mfem::forall(size_4, [=] MFEM_HOST_DEVICE (int l) 
    {
        for (int k = 0; k < size_3; k++)
           for (int j = 0; j < size_2; j++)
              for (int i = 0; i < size_1; i++)
                 d_rho(i,j,k,l) = ...;
    });
\end{lstlisting}
For this case, the GPU will launch \texttt{size\_4} threads, and each will execute all of the \texttt{i}, \texttt{j}, and \texttt{k} instances for a single value of \texttt{l}. One can expose a higher level of parallelism by launching more threads, namely \texttt{size\_1 x size\_2 x size\_ 3 x size\_4}, so that each thread has to execute only a single combination of the \texttt{i}, \texttt{j}, \texttt{k}, and \texttt{l} indices. One can do this as follows
\begin{lstlisting}
    Vector rho;
    rho.UseDevice(true);

    const int size_1 = ...;
    const int size_2 = ...;
    const int size_3 = ...;
    const int size_4 = ...;

    auto d_rho = Reshape(rho.Write(), size_1, size_2, size_3, size_4);

    mfem::forall_3D(size_4, size_1, size_2, size_3, [=] MFEM_HOST_DEVICE (int l) 
    {
        MFEM_FOREACH_THREAD(k, thread_id_3, size_3)
           MFEM_FOREACH_THREAD(j, thread_id_2, size_2)
              MFEM_FOREACH_THREAD(i, thread_id_1, size_1)
                 d_rho(i,j,k,l) = ...;
    });
\end{lstlisting}
In the first example above the GPU launches \texttt{size\_4} / \texttt{MFEM\_CUDA\_BLOCKS} thread blocks, each with \texttt{MFEM\_CUDA\_BLOCKS} threads, for a total of \texttt{size\_4} threads. On the second example, the GPU launches \texttt{size\_4} thread blocks, each with \texttt{size\_1 x size\_2 x size\_ 3} threads, for a total of \texttt{size\_1 x size\_2 x size\_ 3 x size\_4}. 

%------------------------------------------------------------------------
\section{Software vs hardware}
%------------------------------------------------------------------------
First we'll focus on the software side. A thread executes an instance of the kernel. By this we mean the thread can execute an instance of a for-loop (note that this for-loop can have sub for-loops or not). A thread block is a set of concurrently executing threads that can cooperate among themselves through barrier synchronization and shared memory. A thread has a thread ID within its thread block. A grid is an array of thread blocks that execute the same kernel, read inputs from global memory, write results to global memory, and synchronize between dependent kernel calls. A thread block has a block ID within its grid.

On the hardware side, the GPU is formed by many streaming multiprocessors (Nvidia) or compute units (AMD), and each streaming multiprocessor/compute unit contains multiple GPU cores. From the NVIDIA Fermi Compute Architecture Whitepaper:
``a GPU executes one or more kernel grids; a streaming multiprocessor (SM) executes one or more thread blocks;
and CUDA cores and other execution units in the SM execute threads.'' A summary of these terms is shown in the following table

\begin{center}
    \begin{tabular}{c | c }
        %\hline
        Software & Hardware \\
        \hhline{=|=}
        thread & core \\ 
        %\hline
        thread block & SM/CP \\
        %\hline
        grid & GPU \\
        %\hline
    \end{tabular}
\end{center}

%------------------------------------------------------------------------
\section{Debugging}
%------------------------------------------------------------------------

%---------------------------------
\subsection{Nvidia/CUDA}
%---------------------------------

\texttt{cuda-memcheck} and \texttt{compute-sanitizer} are two debugging tools for CUDA, which are essentially the same, with the latter being a newer version of the former. Each tool is essentially four tools, which can be chosen at runtime. The four options are
\begin{itemize}
    \item \texttt{memcheck}
    \item \texttt{syncheck}
    \item \texttt{racecheck}
    \item \texttt{initcheck}
\end{itemize}

%---------------------------------
\subsection{AMD/HIP}
%---------------------------------

%------------------------------------------------------------------------
\section{Profiling}
%------------------------------------------------------------------------

NOTE : For both NVIDIA/CUDA and AMD/HIP, don't forget to limit the number of cycles in the simulations that you want to profile (e.g.\@ by setting the number of time cycles to a sufficiently small number, such as ten or twenty). The more cycles you run, the more cumbersome profilers for both architectures can become, and might not even be able to display the needed results.

%---------------------------------
\subsection{Nvidia/CUDA}
%---------------------------------

We recommend you use \texttt{nsys} (CLI tool) for profiling and \texttt{nsys-ui} (GUI tool) to look at the results but you can use the deprecated nvprof and nvvp tools if you experience issues with \texttt{nsys}. There are two main types of profiling when it comes to CUDA, the timeline profile and the performance counters profile.

\texttt{nsys-ui} supports kernel renaming and can be toggled from the GUI: Tools/Options.../Report Behavior/Rename CUDA Kernels by NVTX.

To use \texttt{nsys} make sure the executable is in your path. If it is not, you can add it by doing \texttt{module load nsight-systems/<some version of nsight-systems>}. An example on how to launch your simulation with \texttt{nsys} is shown below.

\begin{verbatim}
lrun -n 1 \texttt{nsys} profile --output=profile.%q{OMPI_COMM_WORLD_RANK} \
    my_executable my_input_file --caliper nvtx
\end{verbatim}

To visualize the output, launch \texttt{nsys-ui}, click on File in the top left corner, and open up the output file generated from the sample launch command above.

%---------------------------------
\subsection{AMD/HIP}
%---------------------------------

For El Capitan you can use \texttt{rocprof}. Like \texttt{nsys}, \texttt{rocprof} can generate timeline profiles and performance counter profiles.

\texttt{rocprof} supports kernel renaming for timeline profiles with \texttt{ROCP\_RENAME\_KERNEL=1}---either export it or prepend it to your profiling command line.

To use \texttt{rocprof} make sure the executable is in your path. If it is not, you can add it by doing \texttt{module load rocm/<version of rocm used with your code>}. An example on how to launch your simulation with \texttt{rocprof} is shown below.
\begin{verbatim}
ROCP_RENAME_KERNEL=1 flux run -n 1 rocprof --hip-trace --roctx-trace -o profile.csv \
    my_executable my_input_file --caliper roctx
\end{verbatim}

Unlike \texttt{nsys} there is no vendor-provided time viewer so you must use either chrome://tracing in Google Chrome or Perfetto to visualize the output of rocprof. To launch Google Chrome from a terminal simply type \texttt{google-chrome}. Then, type chrome://tracing in the address bar, click on Load in the top left corner, and open up the file profile.json generated from the sample launch command above.

\end{document}
